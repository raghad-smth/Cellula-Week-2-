üñºÔ∏èüìù Text & Image Classifier

üìç Introduction

This project is a simple sentiment analysis app that combines an image (or image caption) with additional user text to predict whether the final combined text is positive or negative, along with a confidence score.
	‚Ä¢	Option 1: Upload an image ‚Üí the app will generate a caption using a multimodal model.
	‚Ä¢	Option 2: Enter an image caption manually.
	‚Ä¢	In both cases, you then provide an additional text input.
	‚Ä¢	The system combines both inputs, runs them through a sentiment classifier, and produces a result with a confidence score.

The goal of this project is mainly educational:
	‚Ä¢	To practice working with different models (image-to-text + text classification).
	‚Ä¢	To experiment with deployment using Streamlit.

‚∏ª

üìç System Architecture

The system is composed of three main parts:

1Ô∏è‚É£ Input Handling (image_caption.py)
	‚Ä¢	Users can upload an image or enter a caption manually.
	‚Ä¢	If an image is uploaded, we use BLIP (Bootstrapping Language-Image Pre-training), a multimodal model that converts images into text.
	‚Ä¢	We used BLIP v1 (lighter & faster than BLIP v2).
	‚Ä¢	Chosen because it can be downloaded and run locally (since Hugging Face does not offer a free API for BLIP).
	‚Ä¢	If a caption is entered manually, we skip this step and use the provided text directly.

2Ô∏è‚É£ Classification (classifier.py)
	‚Ä¢	The caption (generated or manual) is combined with the additional user text.
	‚Ä¢	This combined input is sent to a sentiment analysis model:
	‚Ä¢	cardiffnlp/twitter-roberta-base-sentiment-latest
	‚Ä¢	A variant of RoBERTa pre-trained specifically for sentiment classification.
	‚Ä¢	The output is a label (positive, negative, neutral) and a confidence score.
	‚Ä¢	The model is accessed via the Hugging Face Inference API, so no heavy local downloads are needed.

3Ô∏è‚É£ Data Storage (records.csv)
	‚Ä¢	Every classification result is stored in a CSV file for tracking.
	‚Ä¢	Each row contains:
	‚Ä¢	image_caption
	‚Ä¢	user_text
	‚Ä¢	combined_input
	‚Ä¢	label
	‚Ä¢	score

‚∏ª

üìç User Interface (Streamlit)

The entire app is wrapped in a Streamlit UI for easy interaction:
	‚Ä¢	Upload images / enter captions.
	‚Ä¢	Enter additional text.
	‚Ä¢	View generated captions, sentiment predictions, and scores.
	‚Ä¢	See past records stored in CSV.

Streamlit also provides a shareable local network link so others can test it while your computer is running.

‚∏ª

üìç Installation & Usage

1Ô∏è‚É£ Clone the repository

git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name

2Ô∏è‚É£ Create and activate a virtual environment

python -m venv myenv
source myenv/bin/activate  # Mac/Linux
myenv\Scripts\activate     # Windows

3Ô∏è‚É£ Install dependencies

pip install -r requirements.txt

Key dependencies:
	‚Ä¢	transformers
	‚Ä¢	torch
	‚Ä¢	pandas
	‚Ä¢	streamlit
	‚Ä¢	Pillow

4Ô∏è‚É£ Add your Hugging Face API key

Create a .env file:

HF_TOKEN=your_api_key_here

5Ô∏è‚É£ Run the app

streamlit run classifier.py


‚∏ª

üìç Lessons Learned
	1.	‚úÖ Always check model availability on Hugging Face:
	‚Ä¢	Some models are available via API.
	‚Ä¢	Some must be downloaded locally.
	‚Ä¢	This influences design decisions.
	2.	‚úÖ First time working with Streamlit ‚Äî found it extremely simple and effective for deployment.
	3.	‚úÖ Importance of hiding warnings:
	‚Ä¢	Keep them visible during debugging.
	‚Ä¢	Hide them during deployment for cleaner user experience.
	4.	‚úÖ Learned about BLIP (Bootstrapping Language-Image Pre-training):
	‚Ä¢	Converts images ‚Üí text.
	‚Ä¢	Has many real-world applications (accessibility, content generation, etc.).
	5.	‚úÖ Realized that building real systems often requires multiple models, not just one.
	‚Ä¢	Here, we used BLIP (image ‚Üí text) + RoBERTa (text sentiment classification).
	6.	‚úÖ Sentiment models are often dominated by strong words.
	‚Ä¢	Example: ‚Äúbeautiful sunrise‚Äù + ‚Äúdeath‚Äù ‚Üí likely classified as negative because of word weighting.
	7.	‚úÖ Coding today is less about writing everything from scratch.
	‚Ä¢	It‚Äôs about knowing how to use, adapt, and connect tools/models effectively.
